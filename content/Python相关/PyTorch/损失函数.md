
## 损失函数
### L1 Loss（绝对值损失）：对应拉普拉斯分布
>$l(y,y^{\prime})=|y-y^{\prime}|$

![[Pasted image 20231006183037.png]]
假设真实值y=0时
蓝色：损失值函数；
绿色：似然函数；
橙色：损失函数的梯度，在非0时都是一个长度，会比较稳定，在0时不可导，梯度在-1和1间波动，在优化末期，估计值靠近真实值时会不那么稳定。

### L2 Loss（均方损失）：对应高斯分布
也称均方差损失（Mean Square Error，MSE）

>$l(y,y^{\prime})=\frac12(y-y^{\prime})^2$

![[Pasted image 20231005191406.png]]
假设真实值y=0时
蓝色：L损失值函数的变化情况，是一个凹函数；
绿色：似然函数？？？作用？
橙色：损失函数的梯度，结合吴恩达大佬的机器学习内容J(w,b)对w和b进行求导，为了方便理解设b为0，能发现梯度（对w的导数）时关于w的一元函数，单增；
`特点：y'越靠近0(假设真实值为0)，梯度值越小，下降速度越慢`

而这个“最有可能”，就是概率，就是似然函数的值，也是对应了绿色曲线的顶点。从图可以看到，在y=0的时候，当某个参数使得y'能取值为0，那么这个参数是最有可能接近样本参数的。按我的理解：y固定，y‘变化，蓝色的线代表了y’偏离y的程度？？？

### Huber's Robust Loss：Huber鲁棒损失
>$l(y,y')=\begin{cases}|y-y'|-\frac12&\text{if}|y-y'|>1\\\frac12(y-y')^2&\text{otherwise}\end{cases}$

![[Pasted image 20231006183608.png]]
假设真实值y=0时
蓝色：损失值函数，在离真实值较远下，是一个绝对值函数，在较近下是一个平方损失；
绿色：似然函数形似高斯分布，在0点不会那么尖；
橙色：损失值梯度在估计值离真实值较远时为常数，下降速度固定，在较近时是一个变化的函数，下降速度变慢，这样就解决了L1 Loss在0附近的稳定性问题。


### 损失函数和成本函数
成本函数是用来衡量**模型的预测输出与实际标签之间的差异或误差的函数**。它是一个关于模型参数的函数，表示模型在整个训练数据集上的平均误差。成本函数的目标是最小化模型的误差，以使模型能够更好地拟合训练数据。
损失函数是成本函数的一种特殊情况，通常用于**衡量单个样本或单个预测结果的误差**。损失函数的计算结果是一个标量，表示模型在单个样本或单个预测结果上的误差程度。损失函数的选择通常取决于具体的任务和模型类型。


## 常用距离

### 欧式距离（欧几里得距离）
是在二维和三维空间中的两点之间的真实距离。点x(x1...)和点y(y1...)
d(x,y)=√[(x1-y1)²+(x2-y2)²+…+(xn-yn)²]

### 曼哈顿距离（城市街区距离）
是指在标准的坐标系中，两个点在标准坐标系上的绝对轴距总和。点x(x1...)和点y(y1...)
d(x,y)=|x1-y1|+|x2-y2|+…+|xn-yn|


## 概率模型常用指标
【【10分钟】了解香农熵，交叉熵和KL散度】 https://www.bilibili.com/video/BV1JY411q72n/?share_source=copy_web&vd_source=6ba5a7ec009a0f45fd393fcd989921f7
### 熵
>$H(p)=\sum p_iI_i^p=\sum p_i\log_2(\frac{1}{p_i})=-\sum p_i\log_2(p_i)$

### 交叉熵
是逻辑回归中常用的损失函数。逻辑回归主要解决二分类问题，所以就是0和1的概率判断。
![[Pasted image 20240402170955.png]]


### KL散度（Kullback-Leibler Divergence）
度量两个概率分布之间的差异。交叉熵-熵？

>$$\begin{aligned}
&\begin{aligned}&D(p\|q)=H(p,q)-H(p)=\sum_1p_iI_i^q-\sum p_iI_i^p\\&=\sum p_i\log_2(\frac{1}{q_i})-\sum p_i\log_2(\frac{1}{p_i})\end{aligned} \\
&=\sum p_{i}\mathrm{log}_{2}(\frac{\rho_{i}}{q_{i}})
\end{aligned}$$