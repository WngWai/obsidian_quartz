在PyTorch中，`t.detach()`函数用于创建一个新的张量，该张量与原始张量**共享相同的底层数据**，是同一个数据存储地址，但**不会进行梯度计算**，**没有保留梯度函数**！它可以用于从`计算图`中分离张量，以便在不需要梯度的情况下使用张量的值。

创建一个**不可导的相同张量**参与后续运算，从而阻断计算图的追踪。类似torch.no_grad()的功能。

y = x\*x
u = y.detach()    # 
z = u \* x
```python
t.detach()
```
**示例**：
```python
import torch

# 创建一个需要梯度的张量
tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 使用t.detach()分离张量
detached_tensor = tensor.detach()

# 修改分离后的张量的值
detached_tensor[0] = 10.0

# 查看原始张量和分离后张量的值
print("原始张量：")
print(tensor)
print("\n分离后的张量：")
print(detached_tensor)
```

**输出**：
```python
# 原始张量的数据也发生了改动
原始张量：
tensor([10.,  2.,  3.], requires_grad=True)

分离后的张量：
tensor([10.,  2.,  3.])
```

在上述示例中，我们创建了一个需要梯度的张量 `tensor`，并将其设置为 `requires_grad=True`，以便在计算过程中进行梯度计算。

然后，我们调用 `tensor.detach()` 创建一个新的分离张量 `detached_tensor`，该张量与原始张量共享相同的底层数据，但不会进行梯度计算。

接下来，我们修改了分离后的张量 `detached_tensor` 的值，将第一个元素修改为 `10.0`。

最后，我们打印输出原始张量 `tensor` 和分离后的张量 `detached_tensor` 的值。可以看到，原始张量的值也被修改为 `10.0`，这是因为它们共享相同的底层数据。

使用`t.detach()`函数可以在需要使用张量的值但不需要梯度计算的情况下，创建一个新的张量。这在某些场景下非常有用，例如在推理阶段或者在对抗生成网络（GANs）中获取生成器的输出。

### 进一步探讨从计算图中分离张量的原因
在给定的代码中，调用了`.detach()`方法来从计算图中分离出张量。这样做的目的是为了**避免梯度计算和反向传播**对这些张量的影响。
当你**创建一个张量并进行一系列操作**时，PyTorch会自动构建计算图，用于计算梯度并进行反向传播。计算图包含了所有操作的历史信息，以便在反向传播时计算梯度并更新模型参数。
但是，在某些情况下，你可能只关心张量的值，而不需要计算梯度。这可能是因为你将这些张量用于可视化、保存或其他目的，而不是用于模型的训练或优化。
通过使用`.detach()`方法，你可以从计算图中分离出张量，使其成为一个新的张量，该新张量与原始张量共享相同的底层数据，但不再与计算图相关联。这样，后续的操作将不会对分离的张量计算梯度或进行反向传播。
在给定的代码中，`.detach()`方法被用于将`features[:, 1]`和`labels`分离出来。这可能是因为这些张量将用于可视化的散点图，而不需要计算梯度。这样可以避免不必要的计算和内存消耗。
