https://zhuanlan.zhihu.com/p/40378224
forward前向传播：将上一层的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止;
backward反向传播：仅指用于计算梯度的方法。跟梯度下降算法更新参数不同。

计算图：保留前向传播关系
![[Pasted image 20231002171455.png]]

在反向传播中，计算梯度值！
![[Pasted image 20231002172846.png]]

1，u1、y1...都可以看作单独的神经元，每层将输出结果向前传播；2，在求整体得到的结果最优时，反向传播整体调整参数；

![[Pasted image 20231002173021.png]]

* 反向传播算法(Back-Propagation,BP)，深度学习核心之一；
* 加速计算参数梯度值的方法；
* 在计算机中，反向传播使用计算图(Computation Graphs)的方式进行模块化的运算；

### BP算法详解（Back Propagation）
温故知新——前向传播算法和反向传播算法（BP算法）及其推导 - G-kdom的文章 - 知乎 https://zhuanlan.zhihu.com/p/71892752
**backprop**允许来自代价函数的信息通过网络向后流动，以便计算梯度。
参数更新是反向传播的目的。

`反向传播不是从局限于某一层的神经元参数的调整，而是从整体着手的！从最终输出结果，反向逐层调整每层神经元的参数！以实现整体输出最优`
将每一层的输出进行前向传播，将最后一层的结果与真实值对比的差异进行反向传播。


![[Pasted image 20231213211645.png]]
1，张量的**计算图**：
`节点`表示**可微分的张量**，`边`表示**函数计算关系**。
- 节点类型：
	叶节点：初始输入的可微分张量x；
	中间节点：y；
	输出节点：最后计算得到的张量z；

2，张量计算图的动态性表现为在增删张量的计算关系中，计算图得到增删修改。而静态图是TensorFlow老版的形式，先构建完整的张量结点关系，再进行计算。

3，反向传播：**保留构建关系（张量计算图）**，从而实现反向传播。

.grad属性保留着梯度值（导数）。一般是对输出节点执行反向传播，执行方法z.backward()反向传播操作，从而在x.grad中储存z对x的在指定x值上的梯度值（导数），得到在**叶节点的梯度值**。
非叶节点的梯度是没有的，如y.grad会报错！
在一张计算图上只能执行一次反向传播，不管是对中间节点还是输出节点执行，执行完这张计算图的其他节点就不能再次执行反向传播了。z.backward()只能执行一次，再次反向传播就会报错。需要初始化才能再次执行，执行trainer = optim.SGD(net.parameters(), lr=0.03)，trainer.zero_grad()。
y.backward()，x.grad就只保留了y对x的梯度值！

![[Pasted image 20240423114454.png]]

J(W)是求W的梯度！