
![[Pasted image 20240116095942.png]]


### sigmoid
也称Logistic函数
**取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类**。
>$$f(x)=\frac{1}{1+e^{-x}}$$

![[Pasted image 20240116101803.png]]


算激活函数时（指数运算），**计算量大**，反向传播求误差梯度时很容易就会出现**梯度消失**的情况。在大于0小于1的情况下，多导几次，数值会变得非常小。


![[Pasted image 20240116101714.png]]



### Tanh（Hyperbolic Tangent）
双曲正切函数
>$$\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$


![[Pasted image 20240116201507.png]]

### ReLU（Rectified Linear Unit）
**整流线性单位函数**，又称**修正线性单元**。
`实际就是0和x相比取大值。起了个新名字ReLU！`

>$$f(x)=\max(0,x)$$

![[Pasted image 20240116100505.png]]

ReLu会使一部分神经元的输出为0，，这样就造成了 网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。
也能将线性模型变为非线性模型，能解决更复杂的问题；也可以理解为“激活阈值”，在一定的强度下才传输信息！
